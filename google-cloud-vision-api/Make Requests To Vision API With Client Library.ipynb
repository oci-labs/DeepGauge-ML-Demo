{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Requests To Google Cloud Vision API With Python Client Library\n",
    "## Install the client library\n",
    "```bash\n",
    "$ pip install --upgrade google-cloud-vision\n",
    "```\n",
    "- [Cloud Vision API](https://gcloud-python.readthedocs.io/en/latest/vision/gapic/v1/api.html)\n",
    "\n",
    "## Label detection\n",
    "Now you can use the Vision API to request information from an image, such as label detection. Run the following code to perform your first image label detection request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports stanadard Python libraries\n",
    "import sys, io, os, pprint\n",
    "\n",
    "# Imports the Google OAuth2 service account\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "\n",
    "import argparse\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACE_IMAGE = 'standard-face.jpg'\n",
    "SERVICE_ACCOUNT_FILE = './OCIDeepGauge-eaa8c9248a8d.json'\n",
    "creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates a client\n",
    "client = vision.ImageAnnotatorClient(credentials=creds)\n",
    "\n",
    "# The name of the image file to annotate\n",
    "file_name = os.path.join(os.path.dirname('./'), FACE_IMAGE)\n",
    "\n",
    "# Loads the image into memory\n",
    "with io.open(file_name, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = types.Image(content=content)\n",
    "\n",
    "# Performs label detection on the image file\n",
    "response = client.label_detection(image=image)\n",
    "labels = response.label_annotations\n",
    "\n",
    "print('Labels:')\n",
    "for label in labels:\n",
    "    print(label.description)\n",
    "\n",
    "# pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Draws squares around detected faces in the given image.\"\"\"\n",
    "\"\"\"\n",
    "label_detection(image, options=None, **kwargs)\n",
    "Perform label detection.\n",
    "\n",
    "Parameters: image (Image) – The image to analyze.\n",
    "options (google.gax.CallOptions) – Overrides the default settings for this call, e.g, timeout, retries, etc.\n",
    "kwargs (dict) – Additional properties to be set on the AnnotateImageRequest.\n",
    "Returns: The API response.\n",
    "\n",
    "Return type: AnnotateImageResponse\n",
    "\"\"\"\n",
    "def detect_face(face_file, max_results=4):\n",
    "    \"\"\"Uses the Vision API to detect faces in the given file.\n",
    "\n",
    "    Args:\n",
    "        face_file: A file-like object containing an image with faces.\n",
    "\n",
    "    Returns:\n",
    "        An array of Face objects with information about the picture.\n",
    "    \"\"\"\n",
    "    client = vision.ImageAnnotatorClient(credentials=creds)\n",
    "\n",
    "    content = face_file.read()\n",
    "    image = types.Image(content=content)\n",
    "\n",
    "    return client.face_detection(image=image).face_annotations\n",
    "\n",
    "\n",
    "def highlight_faces(image, faces, output_filename):\n",
    "    \"\"\"Draws a polygon around the faces, then saves to output_filename.\n",
    "\n",
    "    Args:\n",
    "      image: a file containing the image with the faces.\n",
    "      faces: a list of faces found in the file. This should be in the format\n",
    "          returned by the Vision API.\n",
    "      output_filename: the name of the image file to be created, where the\n",
    "          faces have polygons drawn around them.\n",
    "    \"\"\"\n",
    "    im = Image.open(image)\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    # Sepecify the font-family and the font-size\n",
    "#     font = ImageFont.truetype(\"arial.ttf\", 25)\n",
    "    for face in faces:\n",
    "        box = [(vertex.x, vertex.y)\n",
    "               for vertex in face.bounding_poly.vertices]\n",
    "        draw.line(box + [box[0]], width=5, fill='#00ff00')\n",
    "        # Place the confidence value/score of the detected faces above the\n",
    "        # detection box in the output image\n",
    "        draw.text(((face.bounding_poly.vertices)[0].x,\n",
    "                   (face.bounding_poly.vertices)[0].y - 30),\n",
    "                  str(format(face.detection_confidence, '.3f')) + '%',\n",
    "                fill='#FF0000',\n",
    "#                  font=font\n",
    "                 )\n",
    "    im.save(output_filename)\n",
    "\n",
    "\n",
    "def main(input_filename, output_filename, max_results):\n",
    "    with open(input_filename, 'rb') as image:\n",
    "        faces = detect_face(image, max_results)\n",
    "        print('Found {} face{}'.format(\n",
    "            len(faces), '' if len(faces) == 1 else 's'))\n",
    "\n",
    "        print('Writing to file {}'.format(output_filename))\n",
    "        # Reset the file pointer, so we can read the file again\n",
    "        image.seek(0)\n",
    "        highlight_faces(image, faces, output_filename)\n",
    "\n",
    "        \n",
    "        \n",
    "OUTPUT= 'detect-{}'.format(FACE_IMAGE)\n",
    "RESULTS='1'\n",
    "\n",
    "\n",
    "main(FACE_IMAGE, OUTPUT, RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=FACE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Faces\n",
    "Face Detection detects multiple faces within an image along with the associated key facial attributes such as emotional state or wearing headwear. Facial Recognition is not supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(path):\n",
    "    \"\"\"Detects faces in an image.\"\"\"\n",
    "    client = vision.ImageAnnotatorClient(credentials=creds)\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.types.Image(content=content)\n",
    "\n",
    "    response = client.face_detection(image=image)\n",
    "    faces = response.face_annotations\n",
    "    \n",
    "#     pprint.pprint(response)\n",
    "    \n",
    "    # Names of likelihood from google.cloud.vision.enums\n",
    "    likelihood_name = ('UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE', 'LIKELY', 'VERY_LIKELY')\n",
    "    print('Faces:')\n",
    "    \n",
    "    for face in faces:\n",
    "        print('anger: {}'.format(likelihood_name[face.anger_likelihood]))\n",
    "        print('joy: {}'.format(likelihood_name[face.joy_likelihood]))\n",
    "        print('surprise: {}'.format(likelihood_name[face.surprise_likelihood]))\n",
    "        print('sorrow: {}'.format(likelihood_name[face.sorrow_likelihood]))\n",
    "        print('headwear: {}'.format(likelihood_name[face.headwear_likelihood]))\n",
    "        print('blurred: {}'.format(likelihood_name[face.blurred_likelihood]))\n",
    "        print('under exposed: {}'.format(likelihood_name[face.under_exposed_likelihood]))\n",
    "\n",
    "        vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                    for vertex in face.bounding_poly.vertices])\n",
    "\n",
    "        print('face bounds: {}'.format(','.join(vertices)))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_faces(FACE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image_properties(image, options=None, **kwargs)\n",
    "Return image properties information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:  image (Image) – The image to analyze.\n",
    "options (google.gax.CallOptions) – Overrides the default settings for this call, e.g, timeout, retries, etc.\n",
    "kwargs (dict) – Additional properties to be set on the AnnotateImageRequest.\n",
    "\n",
    "Returns:  The API response.\n",
    "\n",
    "Return type:  AnnotateImageResponse\n",
    "\"\"\"\n",
    "\n",
    "path=FACE_IMAGE\n",
    "client = vision.ImageAnnotatorClient(credentials=creds)\n",
    "\n",
    "with io.open(path, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.types.Image(content=content)\n",
    "\n",
    "response = client.image_properties(image=image)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## safe_search_detection(image, options=None, **kwargs)\n",
    "Perform safe search detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters: image (Image) – The image to analyze.\n",
    "options (google.gax.CallOptions) – Overrides the default settings for this call, e.g, timeout, retries, etc.\n",
    "kwargs (dict) – Additional properties to be set on the AnnotateImageRequest.\n",
    "Returns: The API response.\n",
    "\n",
    "Return type: AnnotateImageResponse\n",
    "\"\"\"\n",
    "\n",
    "path=FACE_IMAGE\n",
    "client = vision.ImageAnnotatorClient(credentials=creds)\n",
    "\n",
    "with io.open(path, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.types.Image(content=content)\n",
    "\n",
    "response = client.safe_search_detection(image=image)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## landmark_detection(image, options=None, **kwargs)\n",
    "Perform landmark detection.\n",
    "```\n",
    "Parameters:\t\n",
    "- image (Image) – The image to analyze.\n",
    "- options (google.gax.CallOptions) – Overrides the default settings for this call, e.g, timeout, retries, etc.\n",
    "- kwargs (dict) – Additional properties to be set on the AnnotateImageRequest.\n",
    "\n",
    "Returns: The API response.\n",
    "\n",
    "Return type: AnnotateImageResponse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logo_detection(image, options=None, **kwargs)\n",
    "Perform logo detection.\n",
    "```\n",
    "Parameters:\t\n",
    "- image (Image) – The image to analyze.\n",
    "- options (google.gax.CallOptions) – Overrides the default settings for this call, e.g, timeout, retries, etc.\n",
    "- kwargs (dict) – Additional properties to be set on the AnnotateImageRequest.\n",
    "\n",
    "Returns: The API response.\n",
    "\n",
    "Return type: AnnotateImageResponse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text_detection(image, options=None, **kwargs)#\n",
    "Perform text detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## web_detection(image, options=None, **kwargs)\n",
    "Perform web detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
